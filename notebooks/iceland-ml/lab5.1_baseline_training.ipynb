{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81481c32",
   "metadata": {},
   "source": [
    "# Lab 6: Baseline Model Training (2 Hours)\n",
    "\n",
    "## ‚è±Ô∏è Time Allocation\n",
    "- **Part 1 (40 min):** Model architecture design\n",
    "- **Part 2 (55 min):** Data loading and training implementation\n",
    "- **Part 3 (25 min):** GPU training with SLURM\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "### Core (Essential)\n",
    "- ‚úÖ Build CNN classifier with PyTorch\n",
    "- ‚úÖ Implement DataLoaders for patch datasets\n",
    "- ‚úÖ Create training loop with optimizer\n",
    "- ‚úÖ Submit GPU job via SLURM\n",
    "- ‚úÖ Monitor training and save checkpoints\n",
    "\n",
    "### Optional (For Early Finishers)\n",
    "- üîµ Experiment with different architectures (ResNet, EfficientNet)\n",
    "- üîµ Implement learning rate scheduling\n",
    "- üîµ Add early stopping\n",
    "- üîµ Try mixed precision training\n",
    "- üîµ Set up TensorBoard logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f649d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b566ebbf",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "project_dir = Path(os.getenv('PROJECT_training2600')) / 'my_workspace'\n",
    "data_dir = project_dir / 'data' / 'preprocessed'\n",
    "model_dir = project_dir / 'models'\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Data directory: {data_dir}\")\n",
    "print(f\"üìÇ Model directory: {model_dir}\")\n",
    "\n",
    "# Load data\n",
    "print(\"\\nüì• Loading preprocessed data...\")\n",
    "X_train = np.load(data_dir / 'X_train.npy')\n",
    "y_train = np.load(data_dir / 'y_train.npy')\n",
    "X_val = np.load(data_dir / 'X_val.npy')\n",
    "y_val = np.load(data_dir / 'y_val.npy')\n",
    "\n",
    "# Load metadata\n",
    "with open(data_dir / 'dataset_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded:\")\n",
    "print(f\"   Train: {X_train.shape}\")\n",
    "print(f\"   Val: {X_val.shape}\")\n",
    "print(f\"   Classes: {metadata['num_classes']} ({', '.join(metadata['class_names'])})\")\n",
    "print(f\"   Bands: {metadata['num_bands']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb392b3d",
   "metadata": {},
   "source": [
    "## Section 2: Create PyTorch Dataset and DataLoader (5 min)\n",
    "\n",
    "### Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for satellite imagery patches.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: numpy array of shape (N, C, H, W)\n",
    "            y: numpy array of shape (N,)\n",
    "            transform: optional transforms to apply\n",
    "        \"\"\"\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SatelliteDataset(X_train, y_train)\n",
    "val_dataset = SatelliteDataset(X_val, y_val)\n",
    "\n",
    "print(f\"‚úÖ Created datasets:\")\n",
    "print(f\"   Train: {len(train_dataset)} samples\")\n",
    "print(f\"   Val: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414768c7",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "num_workers = 4  # For parallel data loading\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True  # Faster GPU transfer\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Created DataLoaders:\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "\n",
    "# Test a batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"\\n   Sample batch shape: {images.shape}\")\n",
    "print(f\"   Sample labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce9f71",
   "metadata": {},
   "source": [
    "## Section 3: Build CNN Classifier (8 min)\n",
    "\n",
    "### Simple CNN Architecture\n",
    "We'll start with a baseline CNN, then optionally use pre-trained models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95805908",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteCNN(nn.Module):\n",
    "    \"\"\"Baseline CNN for satellite image classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=6, num_classes=7):\n",
    "        super(SatelliteCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # 224 -> 112\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # 112 -> 56\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # 56 -> 28\n",
    "        )\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)  # 28 -> 14\n",
    "        )\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SatelliteCNN(\n",
    "    in_channels=metadata['num_bands'],\n",
    "    num_classes=metadata['num_classes']\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úÖ Model created and moved to {device}\")\n",
    "print(f\"\\nüìä Model Summary:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7817fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nüî¢ Model Parameters:\")\n",
    "print(f\"   Total: {total_params:,}\")\n",
    "print(f\"   Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504bd0cd",
   "metadata": {},
   "source": [
    "## Section 4: Training Setup (7 min)\n",
    "\n",
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850364c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (CrossEntropyLoss for multi-class classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Adam with weight decay)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler (reduce on plateau)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training components initialized:\")\n",
    "print(f\"   Loss: CrossEntropyLoss\")\n",
    "print(f\"   Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"   Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd1164a",
   "metadata": {},
   "source": [
    "### Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d55c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc='Training', leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validation', leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e7c1c",
   "metadata": {},
   "source": [
    "## Section 5: Train the Model (10 min)\n",
    "\n",
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "early_stop_patience = 5\n",
    "\n",
    "# History tracking\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(f\"üöÄ Starting training for {num_epochs} epochs...\\n\")\n",
    "print(f\"{'Epoch':<8} {'Train Loss':<12} {'Train Acc':<12} {'Val Loss':<12} {'Val Acc':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"{epoch+1:<8} {train_loss:<12.4f} {train_acc:<12.2f} {val_loss:<12.4f} {val_acc:<12.2f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        torch.save(checkpoint, model_dir / 'best_model.pth')\n",
    "        print(f\"   ‚úì Saved best model (val_acc: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"\\n‚ö†Ô∏è Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da133f",
   "metadata": {},
   "source": [
    "## Section 6: Visualize Training Progress (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94faacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "axes[1].plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_dir / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüíæ Saved training curves to: {model_dir / 'training_curves.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12c854",
   "metadata": {},
   "source": [
    "### Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74662b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history as JSON\n",
    "with open(model_dir / 'training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Saved training history to: {model_dir / 'training_history.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f8ef9",
   "metadata": {},
   "source": [
    "## Section 7: SLURM Job Submission (5 min)\n",
    "\n",
    "### Create Training Script\n",
    "For longer training runs, submit as a batch job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this notebook's training code as a standalone script\n",
    "training_script = \"\"\"\n",
    "#!/usr/bin/env python3\n",
    "# Standalone training script for SLURM submission\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# [Include SatelliteDataset and SatelliteCNN classes here]\n",
    "# [Include train_epoch and validate_epoch functions here]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    project_dir = Path(os.getenv('PROJECT_training2600')) / 'my_workspace'\n",
    "    data_dir = project_dir / 'data' / 'preprocessed'\n",
    "    model_dir = project_dir / 'models'\n",
    "    \n",
    "    # [Include full training loop here]\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\"\"\"\n",
    "\n",
    "script_path = project_dir / 'scripts' / 'train_baseline.py'\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "print(f\"üíæ Saved training script to: {script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561cb467",
   "metadata": {},
   "source": [
    "### Create SLURM Submission Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_script = f\"\"\"\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=baseline_train\n",
    "#SBATCH --account=training2600\n",
    "#SBATCH --partition=dc-gpu\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --time=02:00:00\n",
    "#SBATCH --output=logs/train_%j.out\n",
    "#SBATCH --error=logs/train_%j.err\n",
    "\n",
    "# Load modules\n",
    "module load Python/3.11.3\n",
    "module load PyTorch/2.0.1\n",
    "\n",
    "# Activate virtual environment\n",
    "source ~/envs/ml_eo_course/bin/activate\n",
    "\n",
    "# Print GPU info\n",
    "nvidia-smi\n",
    "\n",
    "# Run training\n",
    "python {script_path}\n",
    "\n",
    "echo \"Job finished at $(date)\"\n",
    "\"\"\"\n",
    "\n",
    "sbatch_path = project_dir / 'scripts' / 'submit_training.sbatch'\n",
    "with open(sbatch_path, 'w') as f:\n",
    "    f.write(slurm_script)\n",
    "\n",
    "print(f\"üíæ Saved SLURM script to: {sbatch_path}\")\n",
    "print(f\"\\nüìã To submit job, run:\")\n",
    "print(f\"   cd {project_dir / 'scripts'}\")\n",
    "print(f\"   sbatch submit_training.sbatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b1938a",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What We Covered\n",
    "‚úÖ Built a CNN classifier for satellite imagery  \n",
    "‚úÖ Created PyTorch DataLoaders  \n",
    "‚úÖ Trained model with GPU acceleration  \n",
    "‚úÖ Monitored training metrics  \n",
    "‚úÖ Saved model checkpoints  \n",
    "‚úÖ Created SLURM submission scripts  \n",
    "\n",
    "### Model Performance\n",
    "- **Best Validation Accuracy:** Variable (depends on data)\n",
    "- **Training Time:** ~10-20 min on GPU\n",
    "- **Model Size:** ~5 MB\n",
    "- **Parameters:** ~1-2 million\n",
    "\n",
    "### Key Training Concepts\n",
    "- **Batch Training:** Process data in mini-batches for efficiency\n",
    "- **Learning Rate Scheduling:** Adapt learning rate during training\n",
    "- **Early Stopping:** Prevent overfitting\n",
    "- **Checkpointing:** Save best model based on validation\n",
    "\n",
    "### Prepare for Lab 5.2\n",
    "Next lab: **Model Evaluation Metrics**\n",
    "- Calculate precision, recall, F1-score\n",
    "- Generate confusion matrix\n",
    "- Visualize predictions on test set\n",
    "- Compare against baseline\n",
    "\n",
    "### Best Practices\n",
    "1. **Always monitor both train and val metrics** (detect overfitting)\n",
    "2. **Use GPU for faster training** (10-50x speedup)\n",
    "3. **Save checkpoints regularly** (protect against crashes)\n",
    "4. **Track experiments** (log hyperparameters and results)\n",
    "\n",
    "### Monitoring SLURM Jobs\n",
    "```bash\n",
    "# Check job status\n",
    "squeue -u $USER\n",
    "\n",
    "# View output\n",
    "tail -f logs/train_<job_id>.out\n",
    "\n",
    "# Cancel job\n",
    "scancel <job_id>\n",
    "```\n",
    "\n",
    "### Additional Resources\n",
    "- **PyTorch Tutorials:** https://pytorch.org/tutorials/\n",
    "- **SLURM Docs:** https://slurm.schedmd.com/\n",
    "- **Model Training Best Practices:** https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "### Homework (Optional)\n",
    "1. Experiment with different architectures (ResNet, EfficientNet)\n",
    "2. Try different optimizers (SGD, AdamW)\n",
    "3. Implement data augmentation\n",
    "4. Track experiments with Weights & Biases or TensorBoard\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent progress!** Your baseline model is trained! Next, we'll evaluate its performance in detail. üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa657741",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Lab 6 Completion Checklist\n",
    "\n",
    "### Core Tasks\n",
    "- [ ] CNN model defined and tested\n",
    "- [ ] DataLoader implemented\n",
    "- [ ] Training loop working\n",
    "- [ ] SLURM script created\n",
    "- [ ] GPU job submitted\n",
    "- [ ] Checkpoint saved\n",
    "\n",
    "### Optional Tasks\n",
    "- [ ] Tried different architectures\n",
    "- [ ] Implemented LR scheduling\n",
    "- [ ] Added early stopping\n",
    "- [ ] Set up TensorBoard\n",
    "\n",
    "## üöÄ Next Lab\n",
    "**Lab 7: Model Evaluation** - Load trained model, compute metrics, analyze errors"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
