{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988d2b59",
   "metadata": {},
   "source": [
    "# Lab 7: Model Evaluation (2 Hours)\n",
    "\n",
    "## ‚è±Ô∏è Time Allocation\n",
    "- **Part 1 (30 min):** Load model and generate predictions\n",
    "- **Part 2 (30 min):** Calculate metrics\n",
    "- **Part 3 (30 min):** Confusion matrix and visualization\n",
    "- **Part 4 (30 min):** Error analysis\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "### Core (Essential)\n",
    "- ‚úÖ Load trained model and test data\n",
    "- ‚úÖ Generate predictions\n",
    "- ‚úÖ Compute accuracy, precision, recall, F1\n",
    "- ‚úÖ Create confusion matrix\n",
    "- ‚úÖ Visualize correct/incorrect predictions\n",
    "\n",
    "### Optional (For Early Finishers)\n",
    "- üîµ ROC and PR curves\n",
    "- üîµ Per-class analysis\n",
    "- üîµ Spatial error mapping\n",
    "- üîµ Model comparison\n",
    "- üîµ Statistical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a064de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981968d6",
   "metadata": {},
   "source": [
    "### Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "project_dir = Path(os.getenv('PROJECT_training2600')) / 'my_workspace'\n",
    "data_dir = project_dir / 'data' / 'preprocessed'\n",
    "model_dir = project_dir / 'models'\n",
    "results_dir = project_dir / 'results' / 'evaluation'\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Data directory: {data_dir}\")\n",
    "print(f\"üìÇ Model directory: {model_dir}\")\n",
    "print(f\"üìÇ Results directory: {results_dir}\")\n",
    "\n",
    "# Load test data\n",
    "print(\"\\nüì• Loading test data...\")\n",
    "X_test = np.load(data_dir / 'X_test.npy')\n",
    "y_test = np.load(data_dir / 'y_test.npy')\n",
    "\n",
    "# Load metadata\n",
    "with open(data_dir / 'dataset_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "class_names = metadata['class_names']\n",
    "num_classes = metadata['num_classes']\n",
    "\n",
    "print(f\"\\n‚úÖ Test data loaded:\")\n",
    "print(f\"   Shape: {X_test.shape}\")\n",
    "print(f\"   Classes: {num_classes}\")\n",
    "print(f\"   Samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c5ca2",
   "metadata": {},
   "source": [
    "### Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba5999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define model architecture (must match training)\n",
    "class SatelliteCNN(nn.Module):\n",
    "    \"\"\"Baseline CNN for satellite image classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=6, num_classes=7):\n",
    "        super(SatelliteCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SatelliteCNN(\n",
    "    in_channels=metadata['num_bands'],\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(model_dir / 'best_model.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Model loaded from checkpoint (epoch {checkpoint['epoch']})\")\n",
    "print(f\"   Validation accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4891b",
   "metadata": {},
   "source": [
    "## Section 2: Generate Predictions (5 min)\n",
    "\n",
    "### Create Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b567a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "test_dataset = SatelliteDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"‚úÖ Test DataLoader created ({len(test_loader)} batches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b53b33",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÆ Generating predictions on test set...\\n\")\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc='Inference'):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model(images)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        # Store results\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "print(f\"\\n‚úÖ Predictions generated for {len(all_predictions)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06faf9a3",
   "metadata": {},
   "source": [
    "## Section 3: Calculate Classification Metrics (8 min)\n",
    "\n",
    "### Overall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecebfe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä OVERALL CLASSIFICATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision*100:.2f}%\")\n",
    "print(f\"Recall:    {recall*100:.2f}%\")\n",
    "print(f\"F1-Score:  {f1*100:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729dfb13",
   "metadata": {},
   "source": [
    "### Per-Class Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4094d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "report = classification_report(\n",
    "    all_labels,\n",
    "    all_predictions,\n",
    "    target_names=class_names,\n",
    "    digits=3,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nüìã PER-CLASS CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "with open(results_dir / 'classification_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nüíæ Saved classification report to: {results_dir / 'classification_report.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586b306",
   "metadata": {},
   "source": [
    "### Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "precision_per_class = precision_score(all_labels, all_predictions, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(all_labels, all_predictions, average=None, zero_division=0)\n",
    "f1_per_class = f1_score(all_labels, all_predictions, average=None, zero_division=0)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, precision_per_class, width, label='Precision', alpha=0.8)\n",
    "ax.bar(x, recall_per_class, width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + width, f1_per_class, width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Land Cover Class', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'per_class_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üíæ Saved metrics plot to: {results_dir / 'per_class_metrics.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b8aba",
   "metadata": {},
   "source": [
    "## Section 4: Confusion Matrix (8 min)\n",
    "\n",
    "### Compute Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e4e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(\"üìä Confusion Matrix (counts):\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c896b5",
   "metadata": {},
   "source": [
    "### Visualize Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d49990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize confusion matrix (percentage)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Plot 1: Counts\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    ax=axes[0],\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "\n",
    "# Plot 2: Normalized (percentages)\n",
    "sns.heatmap(\n",
    "    cm_normalized,\n",
    "    annot=True,\n",
    "    fmt='.2%',\n",
    "    cmap='Greens',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    ax=axes[1],\n",
    "    cbar_kws={'label': 'Percentage'}\n",
    ")\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüíæ Saved confusion matrix to: {results_dir / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3005eda",
   "metadata": {},
   "source": [
    "### Analyze Common Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common misclassifications (off-diagonal elements)\n",
    "print(\"\\nüîç Most Common Misclassifications:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "misclassifications = []\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            misclassifications.append({\n",
    "                'true': class_names[i],\n",
    "                'predicted': class_names[j],\n",
    "                'count': cm[i, j],\n",
    "                'percentage': cm_normalized[i, j] * 100\n",
    "            })\n",
    "\n",
    "# Sort by count\n",
    "misclassifications = sorted(misclassifications, key=lambda x: x['count'], reverse=True)\n",
    "\n",
    "for idx, mc in enumerate(misclassifications[:5], 1):\n",
    "    print(f\"{idx}. {mc['true']} ‚Üí {mc['predicted']}: \"\n",
    "          f\"{mc['count']} samples ({mc['percentage']:.1f}% of {mc['true']})\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a93326c",
   "metadata": {},
   "source": [
    "## Section 5: Visualize Predictions (9 min)\n",
    "\n",
    "### Display Correct Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correctly classified samples\n",
    "correct_indices = np.where(all_predictions == all_labels)[0]\n",
    "\n",
    "# Helper function to display RGB\n",
    "def normalize_for_display(img, percentile=2):\n",
    "    vmin, vmax = np.percentile(img, [percentile, 100-percentile])\n",
    "    return np.clip((img - vmin) / (vmax - vmin), 0, 1)\n",
    "\n",
    "# Plot 6 correct predictions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, ax_idx in enumerate(np.random.choice(correct_indices, 6, replace=False)):\n",
    "    img = X_test[ax_idx]\n",
    "    rgb = img[[2, 1, 0], :, :].transpose(1, 2, 0)  # B4, B3, B2\n",
    "    rgb_display = normalize_for_display(rgb)\n",
    "    \n",
    "    true_label = class_names[all_labels[ax_idx]]\n",
    "    pred_label = class_names[all_predictions[ax_idx]]\n",
    "    confidence = all_probabilities[ax_idx][all_predictions[ax_idx]] * 100\n",
    "    \n",
    "    axes[idx].imshow(rgb_display)\n",
    "    axes[idx].set_title(\n",
    "        f\"‚úì True: {true_label}\\nPred: {pred_label} ({confidence:.1f}%)\",\n",
    "        fontsize=10,\n",
    "        color='green'\n",
    "    )\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Correctly Classified Samples', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'correct_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üíæ Saved correct predictions to: {results_dir / 'correct_predictions.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c592dba",
   "metadata": {},
   "source": [
    "### Display Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find incorrectly classified samples\n",
    "incorrect_indices = np.where(all_predictions != all_labels)[0]\n",
    "\n",
    "if len(incorrect_indices) > 0:\n",
    "    # Plot 6 incorrect predictions\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    sample_size = min(6, len(incorrect_indices))\n",
    "    for idx, ax_idx in enumerate(np.random.choice(incorrect_indices, sample_size, replace=False)):\n",
    "        img = X_test[ax_idx]\n",
    "        rgb = img[[2, 1, 0], :, :].transpose(1, 2, 0)\n",
    "        rgb_display = normalize_for_display(rgb)\n",
    "        \n",
    "        true_label = class_names[all_labels[ax_idx]]\n",
    "        pred_label = class_names[all_predictions[ax_idx]]\n",
    "        confidence = all_probabilities[ax_idx][all_predictions[ax_idx]] * 100\n",
    "        \n",
    "        axes[idx].imshow(rgb_display)\n",
    "        axes[idx].set_title(\n",
    "            f\"‚úó True: {true_label}\\nPred: {pred_label} ({confidence:.1f}%)\",\n",
    "            fontsize=10,\n",
    "            color='red'\n",
    "        )\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Misclassified Samples', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'incorrect_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üíæ Saved incorrect predictions to: {results_dir / 'incorrect_predictions.png'}\")\n",
    "else:\n",
    "    print(\"üéâ Perfect classification! No errors to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168596e6",
   "metadata": {},
   "source": [
    "## Section 6: Performance Summary (5 min)\n",
    "\n",
    "### Class-Wise Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4aa165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dataframe\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = (all_labels == i)\n",
    "    class_count = class_mask.sum()\n",
    "    correct_count = ((all_labels == i) & (all_predictions == i)).sum()\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Class': class_name,\n",
    "        'Samples': class_count,\n",
    "        'Correct': correct_count,\n",
    "        'Precision': precision_per_class[i],\n",
    "        'Recall': recall_per_class[i],\n",
    "        'F1-Score': f1_per_class[i]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\nüìä CLASS-WISE PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(results_dir / 'class_performance_summary.csv', index=False)\n",
    "print(f\"\\nüíæ Saved summary to: {results_dir / 'class_performance_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b657e",
   "metadata": {},
   "source": [
    "### Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation report\n",
    "report_content = f\"\"\"\n",
    "{'='*80}\n",
    "MODEL EVALUATION REPORT\n",
    "{'='*80}\n",
    "\n",
    "DATASET INFORMATION\n",
    "{'-'*80}\n",
    "Test Samples: {len(X_test)}\n",
    "Classes: {num_classes}\n",
    "Class Names: {', '.join(class_names)}\n",
    "Image Size: {X_test.shape[2]}x{X_test.shape[3]} pixels\n",
    "Bands: {X_test.shape[1]}\n",
    "\n",
    "OVERALL METRICS\n",
    "{'-'*80}\n",
    "Accuracy:  {accuracy*100:.2f}%\n",
    "Precision: {precision*100:.2f}%\n",
    "Recall:    {recall*100:.2f}%\n",
    "F1-Score:  {f1*100:.2f}%\n",
    "\n",
    "BEST PERFORMING CLASSES\n",
    "{'-'*80}\n",
    "\"\"\"\n",
    "\n",
    "# Add top 3 classes by F1-score\n",
    "top_classes = summary_df.nlargest(3, 'F1-Score')\n",
    "for _, row in top_classes.iterrows():\n",
    "    report_content += f\"{row['Class']:15s} - F1: {row['F1-Score']:.3f}, Samples: {row['Samples']}\\n\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "WORST PERFORMING CLASSES\n",
    "{'-'*80}\n",
    "\"\"\"\n",
    "\n",
    "# Add bottom 3 classes by F1-score\n",
    "worst_classes = summary_df.nsmallest(3, 'F1-Score')\n",
    "for _, row in worst_classes.iterrows():\n",
    "    report_content += f\"{row['Class']:15s} - F1: {row['F1-Score']:.3f}, Samples: {row['Samples']}\\n\"\n",
    "\n",
    "report_content += f\"\"\"\n",
    "KEY FINDINGS\n",
    "{'-'*80}\n",
    "- Total correct predictions: {(all_predictions == all_labels).sum()} / {len(all_labels)}\n",
    "- Total misclassifications: {(all_predictions != all_labels).sum()}\n",
    "- Most confused pair: {misclassifications[0]['true']} ‚Üí {misclassifications[0]['predicted']} ({misclassifications[0]['count']} cases)\n",
    "\n",
    "RECOMMENDATIONS\n",
    "{'-'*80}\n",
    "1. Classes with low F1-scores may need more training samples\n",
    "2. Consider data augmentation for underrepresented classes\n",
    "3. Analyze misclassified samples for common patterns\n",
    "4. Experiment with different architectures or pre-trained models\n",
    "\n",
    "{'='*80}\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "with open(results_dir / 'evaluation_report.txt', 'w') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(report_content)\n",
    "print(f\"\\nüíæ Saved evaluation report to: {results_dir / 'evaluation_report.txt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb2cf2",
   "metadata": {},
   "source": [
    "## Summary & Course Conclusion\n",
    "\n",
    "### What We Covered in This Lab\n",
    "‚úÖ Loaded trained model and test data  \n",
    "‚úÖ Generated predictions on test set  \n",
    "‚úÖ Calculated classification metrics  \n",
    "‚úÖ Visualized confusion matrices  \n",
    "‚úÖ Analyzed model strengths and weaknesses  \n",
    "‚úÖ Created comprehensive evaluation report  \n",
    "\n",
    "### Course Journey Recap\n",
    "Over 6 labs, we covered the complete ML pipeline for Earth Observation:\n",
    "\n",
    "1. **Lab 1:** HPC access and Judoor setup\n",
    "2. **Lab 2:** Jupyter-JSC and Git version control\n",
    "3. **Lab 3:** Sentinel-2 data acquisition with GEE\n",
    "4. **Lab 4:** Data preprocessing and patch extraction\n",
    "5. **Lab 5.1:** Baseline CNN model training\n",
    "6. **Lab 5.2:** Comprehensive model evaluation (this lab)\n",
    "\n",
    "### Key Evaluation Concepts\n",
    "- **Accuracy:** Overall correctness (can be misleading with imbalanced data)\n",
    "- **Precision:** Of predicted positives, how many are actually positive?\n",
    "- **Recall:** Of actual positives, how many did we find?\n",
    "- **F1-Score:** Harmonic mean of precision and recall\n",
    "- **Confusion Matrix:** Visualizes prediction patterns\n",
    "\n",
    "### When to Use Each Metric\n",
    "- **Accuracy:** Overall performance with balanced classes\n",
    "- **Precision:** When false positives are costly (e.g., urban detection)\n",
    "- **Recall:** When false negatives are costly (e.g., fire detection)\n",
    "- **F1-Score:** General balance, especially with imbalanced classes\n",
    "\n",
    "### Next Steps for Your Projects\n",
    "1. **Improve Model Performance:**\n",
    "   - Try pre-trained models (ResNet, EfficientNet)\n",
    "   - Implement data augmentation\n",
    "   - Use foundation models (TerraTorch, Prithvi)\n",
    "   - Ensemble multiple models\n",
    "\n",
    "2. **Expand Dataset:**\n",
    "   - Acquire more Sentinel-2 scenes\n",
    "   - Include temporal information (time series)\n",
    "   - Add auxiliary data (elevation, climate)\n",
    "\n",
    "3. **Advanced Techniques:**\n",
    "   - Semantic segmentation (pixel-level classification)\n",
    "   - Change detection (compare multiple dates)\n",
    "   - Uncertainty estimation\n",
    "   - Active learning\n",
    "\n",
    "4. **Production Deployment:**\n",
    "   - Export model to ONNX for faster inference\n",
    "   - Create web interface with Flask/FastAPI\n",
    "   - Scale inference with distributed computing\n",
    "   - Monitor model performance over time\n",
    "\n",
    "### Additional Resources\n",
    "- **Metrics Tutorial:** https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "- **Confusion Matrix:** https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "- **Foundation Models:** https://github.com/NASA-IMPACT/terratorch\n",
    "- **EO Datasets:** https://github.com/satellite-image-deep-learning/datasets\n",
    "\n",
    "### Course Feedback\n",
    "Please share your feedback:\n",
    "- What worked well?\n",
    "- What could be improved?\n",
    "- Topics you'd like to explore further?\n",
    "\n",
    "### Stay Connected\n",
    "- **Course Slack:** Continue discussions and share results\n",
    "- **GitHub:** Share your code and projects\n",
    "- **Email:** s.hashim@fz-juelich.de for questions\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Congratulations!\n",
    "\n",
    "You've successfully completed the Machine Learning for Earth Observation course!\n",
    "\n",
    "You now have the skills to:\n",
    "- ‚úÖ Access and process satellite imagery\n",
    "- ‚úÖ Build and train deep learning models\n",
    "- ‚úÖ Evaluate model performance rigorously\n",
    "- ‚úÖ Deploy models on HPC infrastructure\n",
    "\n",
    "**Keep learning, keep building, and keep exploring!** üöÄüõ∞Ô∏èüåç"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b0198",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Lab 7 Completion Checklist\n",
    "\n",
    "### Core Tasks\n",
    "- [ ] Model loaded successfully\n",
    "- [ ] Predictions generated\n",
    "- [ ] Metrics calculated (accuracy, precision, recall, F1)\n",
    "- [ ] Confusion matrix created\n",
    "- [ ] Sample visualizations shown\n",
    "\n",
    "### Optional Tasks\n",
    "- [ ] ROC/PR curves plotted\n",
    "- [ ] Per-class deep dive completed\n",
    "- [ ] Spatial analysis performed\n",
    "- [ ] Comparison with baseline\n",
    "\n",
    "## üöÄ Next Lab\n",
    "**Lab 8: TerraTorch Fine-tuning** - Use foundation models for better performance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
